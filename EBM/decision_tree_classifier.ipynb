{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "dataset = [\n",
    "\t[2.771244718, 1.784783929, 0],\n",
    "\t[1.728571309, 1.169761413, 0],\n",
    "\t[3.678319846, 2.81281357, 0],\n",
    "\t[3.961043357, 2.61995032, 0],\n",
    "\t[2.999208922, 2.209014212, 0],\n",
    "\t[7.497545867, 3.162953546, 1],\n",
    "\t[9.00220326, 3.339047188, 1],\n",
    "\t[7.444542326, 0.476683375, 1],\n",
    "\t[10.12493903, 3.234550982, 1],\n",
    "\t[6.642287351, 3.319983761, 1]\n",
    "]\n",
    "# inspired by https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.882914Z",
     "start_time": "2024-02-03T09:29:32.879213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Calculate the Gini index for a split dataset --> The lower it is, the better the data has been split.\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group)) # You could divide it into more than 2 groups, a non-binary tree (less common).\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weighting the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.897678Z",
     "start_time": "2024-02-03T09:29:32.886846Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value: # Convention: left is smaller and right is larger.\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_groups, b_score = None, None, None, math.inf\n",
    "\tfor index in range(len(dataset[0]) - 1): # The last feature is the class, in these datasets - do not consider it.\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset) # Splitting on the data already present in the dataset, efficient.\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index': b_index, 'value': b_value, 'groups': b_groups} # This is the best split, you apply it recursively.\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group): # I set the most frequently occurring class in the last split as the final class.\n",
    "\toutcomes = [row[-1] for row in group] # The class is in the last feature of the row, denoted by row[-1].\n",
    "\n",
    "\tn_class = 2\n",
    "\tcounter = [0 for _ in range(n_class)]\n",
    "\tfor out in outcomes:\n",
    "\t\tcounter[out] += 1\n",
    "\treturn np.argmax(counter)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth): # You don't return anything, but recursively modify the root (parameter node).\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups']) # They won't be needed anymore, you'll only use them to split during training.\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left)\n",
    "\t\tsplit(node['left'], max_depth, min_size, depth + 1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right)\n",
    "\t\tsplit(node['right'], max_depth, min_size, depth + 1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "\troot = get_split(train)\n",
    "\tsplit(root, max_depth, min_size, 1)\n",
    "\treturn root\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "\tif isinstance(node, dict):\n",
    "\t\tprint('%s[X%d < %.3f]' % (depth * '  ', (node['index'] + 1), node['value']))\n",
    "\t\tprint_tree(node['left'], depth + 1)\n",
    "\t\tprint_tree(node['right'], depth + 1)\n",
    "\telse:\n",
    "\t\tprint('%s[%s]' % (depth * '  ', node))\n",
    "\n",
    "tree = build_tree(dataset, 3, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.909302Z",
     "start_time": "2024-02-03T09:29:32.906043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Make a prediction with a decision tree --> You could easily perform multi-class classification as well.\n",
    "def predict(node, row):\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.915966Z",
     "start_time": "2024-02-03T09:29:32.912362Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected = 0, Got = 0\n"
     ]
    }
   ],
   "source": [
    "stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0} # feature, pred_greater, split_val, pred_smaller\n",
    "# This is the structure of a terminal node of the tree - if 'right' and 'left' were nodes themselves, they would be intermediate nodes.\n",
    "prediction = predict(tree, dataset[0][0:2])\n",
    "print('Expected = %d, Got = %d' % (dataset[0][-1], prediction))\n",
    "# An example of prediction on the first sample."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.944712Z",
     "start_time": "2024-02-03T09:29:32.918257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "'''Here is an example of a decision tree classifier - it works well for these simple datasets (both numeric and categorical). The main advantages are explainability and scalability. It can be enhanced with Bagging (to reduce Overfitting) and Boosting (to reduce Underfitting). The most famous example of Bagging is the Random Forest algorithm - it combines (through averaging) the decisions of N decision trees.'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.952572Z",
     "start_time": "2024-02-03T09:29:32.924132Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X1 < 6.642]\n",
      "  [X1 < 2.771]\n",
      "    [0]\n",
      "    [X1 < 2.771]\n",
      "      [0]\n",
      "      [0]\n",
      "  [X1 < 7.498]\n",
      "    [X1 < 7.445]\n",
      "      [1]\n",
      "      [1]\n",
      "    [X1 < 7.498]\n",
      "      [1]\n",
      "      [1]\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree) # with this data you can draw the decision tree and understand the subdivisions\n",
    "# In this case, it is evident that the variable X2 is not useful - you consistently chose to split on X1."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T09:29:32.952971Z",
     "start_time": "2024-02-03T09:29:32.929277Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
